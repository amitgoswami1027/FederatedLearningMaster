# Federated Learning - Distributed Machine Learning !!

# Federated Learning !! Es ist Herausforduerung !!

Federated Learning is born at the intersection of on-device AI, blockchain, and edge computing/IoT. Es ist nich herausfordurung !!

Here we train a centralized Machine Learning model on decentralized data! 

Zum Beispiel !!
Suppose, you got selected as a machine learning intern in a company, and your task is to create a robust machine learning application, that needs to train itself on user-sensitive data.

If we are allowed to extract user data, aggregate it from many users and stack them up on a centralized cloud server, for machine learning model training. We are doing a good job !! Kudos we are done let go home, problem solved.

But Wait, when you submitted the work to Business, there is an update from your Business, we cannot get the user data. Are you not invading someone's Privacy?? Is this was not factored-in in the feasibility assessment of the feature. It's Friday evening and already but a lot of hard work whole week to complete the model. Es ist enttauschened !! 

While taking a sip of Beer on Friday evening and reading some blogs on the topic, thought that it done not seems ethical for sharing the User's data and definately have the privcy concerns. Started thinking some out of box approach to make this happen !!

Can we train our models locally on our respective devices? Is it possible? What about the computation power of our devices?? Will it support to train the neural network models?? Even if the models are trained how we will be integrating the different federated learning model and aggregrate there learnings?? Can we not reverse engineer the model to get the user data in the centralized server?? How can we avoid that to happen??

Thought is - Can we train the model on the devices itself and not on the centralized server. he local data generated by the user history, on a particular device, will now be used as on-device data to train our model and make it smarter, much quicker.!! 

## Here goes the plan.....
* Centralized machine learning application will have a local copy on all devices, where users can use them according to our need.
* The model will now gradually learn and train itself on the information inputted by the user and become smarter, time to time.
* The devices are then allowed to transfer the training results, from the local copy of the machine learning app, back to the central server.

### Only results, not data!!
* This same process happens across several devices, that have a local copy of the application. The results will be aggregated together in the centralized server, this time without user data.
* The centralized cloud server now updates its central machine learning model from the aggregated training results, which is now far better than the previously deployed version.
* The development team now updates the model to a newer version, and users update the application with the smarter model, created from their own data!

### Cool. That’s a lot of awesomeness !!

# Wisdom
Traditional machine learning involves a data pipeline that uses a central server (on-prem or cloud) that hosts the trained model in order to make predictions. The downside of this architecture is that all the data collected by local devices and sensors are sent back to the central server for processing, and subsequently returned back to the devices. This round-trip limits a model’s ability to learn in real-time.

Federated learning (FL) in contrast, is an approach that downloads the current model and computes an updated model at the device itself (ala edge computing) using local data. These locally trained models are then sent from the devices back to the central server where they are aggregated, i.e. averaging weights, and then a single consolidated and improved global model is sent back to the devices.

#### Your phone personalizes the model locally, based on your usage (A). Many users’ updates are aggregated (B) to form a consensus change © to the shared model, after which the procedure is repeated.
![image](https://user-images.githubusercontent.com/13011167/135709032-9785e570-1390-418a-949d-b424c37ad347.png)


Federated Learning seems to have a lot of potentials.Not only it secures user sensitive information, but also aggregates results and identifies common patterns from a lot of users, which makes the model robust, day by day. Federated Learning is still in its early stages and faces numerous challenges with its design and deployment. t trains itself as per its user data, keeps it secure, and then comes back as a smarter guy, which is again ready to test itself from its own user! Training and testing became smarter!

Be it training, testing, or information privacy, Federated Learning created a new era of secured AI.A good way to tackle this challenge is by defining the Federated Learning problem and designing a data pipeline such that it can be properly productionized.

## Google describes how FL works in this way with respect to mobile phones:
It works like this: your device downloads the current model, improves it by learning from data on your phone, and then summarizes the changes as a small focused update. Only this update to the model is sent to the cloud, using encrypted communication, where it is immediately averaged with other user updates to improve the shared model. All the training data remains on your device, and no individual updates are stored in the cloud.

# BENEFITS
* FL enables devices like mobile phones to collaboratively learn a shared prediction model while keeping the training data on the device instead of requiring the data to be uploaded and stored on a central server.
* Moves model training to the edge, namely devices such as smartphones, tablets, IoT, or even “organizations” like hospitals that are required to operate under strict privacy constraints
* Makes real-time prediction possible, since prediction happens on the device itself. FL reduces the time lag that occurs due to transmitting raw data back to a central server
* Since the models reside on the device, the prediction process works even when there is no internet connectivity.
* FL reduces the amount of hardware infrastructure required. FL uses minimal hardware and what is available in mobile devices is more than enough to run the FL models.


# CHALLANGES
* irst, communication is a critical bottleneck in FL networks where data generated on each device remain local. In order to train a model using data generated by the devices in the network, it is necessary to develop communication-efficient methods that reduce the total number of communication rounds, and also iteratively send small model updates as part of the training process, as opposed to sending the entire data set.
* FL methods must: anticipate low levels of device participation, i.e. only a small fraction of the devices being active at once; tolerate variability in hardware that affects storage, computational, and communication capabilities of each device in a federated network; and be able to handle dropped devices in the network.
* Finally, FL helps to protect data generated on a device by sharing model updates such as gradient data instead of raw data. But communicating model updates throughout the training process can still reveal sensitive information, either to a third party, or to the central server.



















## Important Links
* https://www.analyticsvidhya.com/blog/2021/05/federated-learning-a-beginners-guide/
* https://medium.com/@ODSC/what-is-federated-learning-99c7fc9bc4f5
* https://ai.googleblog.com/2017/04/federated-learning-collaborative.html
